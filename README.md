
# ETL Vaca Muerta: Pandas vs Spark

Este proyecto implementa un flujo **ETL (Extract, Transform, Load)** sobre un dataset real de fracturas hidr√°ulicas en **Vaca Muerta** (cuenca neuquina, Argentina).  
Se compara el uso de **Pandas** (para datasets peque√±os) y **Apache Spark** (para big data distribuido).

---

## Quickstart

### 1. Clonar el repositorio
```bash
git clone https://github.com/tu_usuario/vaca-muerta-pipeline.git
cd vaca-muerta-pipeline
2. Crear y activar entorno virtual
bash
Copiar c√≥digo
python3 -m venv .venv
source .venv/bin/activate   # Linux / Mac
# .venv\Scripts\activate    # Windows PowerShell
3. Instalar dependencias
bash
Copiar c√≥digo
pip install -r requirements.txt
4. Ejecutar el ETL con Pandas
bash
Copiar c√≥digo
python src/etl_pandas.py
5. Ejecutar el ETL con Spark
bash
Copiar c√≥digo
python src/etl_spark.py
6. Explorar resultados con Spark
bash
Copiar c√≥digo
python src/read_spark.py
Los datos procesados quedan en:

bash
Copiar c√≥digo
data/processed/fracturas_spark/
Particionados por anio y mes, en formato Parquet listo para an√°lisis o dashboards.

Estructura del proyecto
graphql
Copiar c√≥digo
vaca-muerta-pipeline/
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ raw/                  # Archivos CSV originales
‚îÇ   ‚îî‚îÄ‚îÄ processed/            # Datos procesados (Parquet)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ etl_pandas.py         # Mini ETL con Pandas
‚îÇ   ‚îú‚îÄ‚îÄ etl_spark.py          # ETL distribuido con Spark
‚îÇ   ‚îî‚îÄ‚îÄ read_spark.py         # Exploraci√≥n y an√°lisis con Spark
‚îú‚îÄ‚îÄ logs/                     # Logs de ejecuci√≥n
‚îú‚îÄ‚îÄ requirements.txt          # Dependencias del proyecto
‚îî‚îÄ‚îÄ README.md                 # Documentaci√≥n del proyecto
Tecnolog√≠as utilizadas
Python 3.10

Pandas 2.x ‚Üí ETL local sobre CSV

Apache Spark 4.x (PySpark) ‚Üí ETL distribuido y escritura en Parquet

Parquet ‚Üí formato columnar, comprimido y optimizado para big data

Flujo ETL
1. Extracci√≥n
Se parte de un CSV con datos de fracturas hidr√°ulicas:

idpozo, yacimiento, formacion_productiva, fecha_inicio_fractura, fecha_fin_fractura,

arena_bombeada_nacional_tn, arena_bombeada_importada_tn, agua_inyectada_m3, presion_maxima_psi, etc.

2. Transformaci√≥n
Conversi√≥n de fechas (fecha_inicio_fractura, fecha_fin_fractura)

C√°lculo de columna derivada:

python
Copiar c√≥digo
arena_total_tn = arena_bombeada_nacional_tn + arena_bombeada_importada_tn
Creaci√≥n de columnas de partici√≥n: anio, mes

3. Carga
Con Pandas ‚Üí escritura en CSV procesado en data/processed/.

Con Spark ‚Üí escritura en Parquet particionado en data/processed/fracturas_spark/.

Comparativa Pandas vs Spark
Aspecto	Pandas üêº	Spark ‚ö°
Escalabilidad	Memoria local (GBs)	Distribuido (cluster, TBs+)
Facilidad uso	Muy sencillo, sintaxis Python	Requiere configuraci√≥n de entorno
Formatos	CSV, Excel, simples	Parquet, ORC, Avro, Hive, etc.
Rendimiento	Bueno para datasets chicos	Optimizado para big data
Uso ideal	Exploraci√≥n r√°pida, prototipos	Producci√≥n, ETL masivo, pipelines

Resultados ETL Spark
Estad√≠sticas descriptivas
Arena total promedio: ~4.269 tn

Agua inyectada promedio: ~27.000 m¬≥ (m√°ximo outlier ~537.000 m¬≥)

Presi√≥n m√°xima promedio: ~8.825 psi (outlier >200.000 psi ‚Üí error de carga)

Top yacimientos con m√°s fracturas
LOMA CAMPANA-LLL (586)

EL TORDILLO (365)

LA AMARGA CHICA (270)

LOMA CAMPANA (185)

EL TRAPIAL (182)

Evoluci√≥n temporal
Inicio en 2006 con muy baja actividad.

Crecimiento sostenido entre 2010‚Äì2019 (pico en 2015‚Äì2019 con ~350 fracturas/a√±o).

Fuerte ca√≠da en 2020 (~118 fracturas) ‚Üí impacto de la pandemia.

Recuperaci√≥n 2021‚Äì2023 (370 fracturas/a√±o).

Aprendizajes clave (estilo MIT)
CSV vs Parquet: CSV es simple pero pesado; Parquet permite compresi√≥n, esquema y lecturas r√°pidas.

Pandas vs Spark: Pandas es ideal para desarrollo local; Spark es la elecci√≥n cuando hablamos de escala y producci√≥n.

EDA en Spark: se pueden realizar estad√≠sticas y agrupaciones distribuidas, con el mismo esp√≠ritu que Pandas.

Outliers: la data real de campo presenta errores extremos ‚Üí limpieza y validaci√≥n son esenciales antes de an√°lisis.

Pr√≥ximos pasos
Agregar validaciones de calidad de datos (detecci√≥n autom√°tica de outliers).

Integrar Airflow para orquestar el pipeline.

Crear dashboard en Power BI / Superset para visualizaci√≥n.

Publicar dataset procesado en un data lake (ej: MinIO o S3).

Requisitos
El proyecto requiere las siguientes dependencias (ver requirements.txt):

txt
Copiar c√≥digo
pandas==2.3.2
pyarrow==21.0.0
fastparquet==2024.11.0
pyspark==4.0.1
numpy>=1.26
yaml
Copiar c√≥digo
